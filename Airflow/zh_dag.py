import pandas as pdfrom bs4 import BeautifulSoupfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.common.by import Byimport refrom datetime import timedeltafrom airflow import DAGfrom airflow.operators.python_operator import PythonOperatorfrom airflow.utils.dates import days_agourl = 'https://gauchazh.clicrbs.com.br/ultimas-noticias/'"""    input = A website you want to scrape    outputs = a object that gives you options on how the scraping can happen."""class Scraper:    url = ""    def __init__ (self, url):        self.url = url    def get_firefox_driver(self):        return webdriver.Firefox(executable_path = r'/Users/viniciusroratto/Desktop/Malta/geckodriver')    def get_phantomjs_driver(self, tor = False):        if tor:            service_args = ['--proxy=localhost:9150','--proxy-type=socks5',]            return webdriver.PhantomJS(executable_path=r'/Users/viniciusroratto/Desktop/Malta/phantomjs-2.1.1-macosx/bin/phantomjs', service_args = service_args)        else:            return  webdriver.PhantomJS(executable_path=r'/Users/viniciusroratto/Desktop/Malta/phantomjs-2.1.1-macosx/bin/phantomjs')    def get_html (self, driver, timeout = 10):        driver.get(self.url)        try:            element_present = EC.presence_of_element_located((By.CLASS_NAME, 'section-latest-page'))            WebDriverWait(driver, timeout).until(element_present)        except TimeoutException:            print("Timed out waiting for page to load")        return driver.page_source        def get_titles(self, html, file_path = './'):        soup = BeautifulSoup(html, 'lxml')        table = soup.find_all('table')        company_list = table[0].findAll("tr")        table_title = company_list[0].text + " Under Treshold A"        company = []        link= []        for each in table[0].findAll("td"):            company.append(each.get_text())            link.append(re.split('[()]', str(each.span))[1])                    df = pd.DataFrame(list(zip(company,link)), columns=["Company", "Link"])        df.to_csv(file_path + table_title  + ".csv")        print(df.head())        return df        class ScraperZ (Scraper):        def get_titles(self, html, file_path = './'):        soup = BeautifulSoup(html, 'lxml')        title_list = soup.findAll("h2", {"class" : "m-crd-pt__headline"})        for each in title_list:                print("\n" + each.get_text() + '\n')        return pd.Dataframe(title_list)            """#### How to use:    1) Instantiante the object.    2) You can choose a selenium driver between firefox and phantomjs.        a) You can actively choose to use the TOR network in the phantomJs driver    3) Retrieve webpage's HTML with the get_html method.    4) Retrieve the dataframe using the get_company_list method.        a) A CSV will be saved to this folders, or an optional folder using the file_path parameter."""def run_ZH (url):    titles  = ScraperZ(url)     driver = titles.get_firefox_driver()    html = titles.get_html(driver)    df = titles.get_titles(html)    driver.close()"""Airflow Code: using example from """default_args = {    'owner': 'airflow',    'depends_on_past': False,    'start_date': days_ago(0, 0, 0),    'email': ['airflow@example.com'],    'email_on_failure': False,    'email_on_retry': False,    'retries': 1,    'retry_delay': timedelta(minutes=1)}dag = DAG(    'ZH_DAG',    default_args=default_args,    description='Our first DAG with ETL process!',    schedule_interval=timedelta(minutes=10),)def just_a_function():    print("I'm going to show you something :)")run_dag = PythonOperator(    task_id='whole_ZH',    python_callable=run_ZH,    dag=dag,)run_dag